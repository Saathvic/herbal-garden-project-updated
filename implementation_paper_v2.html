<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AI-Powered Virtual Herbal Garden - Implementation Paper</title>
<!-- KaTeX for equation rendering -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body, {delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}]});"></script>
<style>
  @page {
    size: A4;
    margin: 1.8cm 1.6cm 1.8cm 1.6cm;
  }

  * {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
  }

  body {
    font-family: 'Times New Roman', Times, serif;
    font-size: 10pt;
    line-height: 1.3;
    color: #000;
    background: #fff;
    max-width: 21cm;
    margin: 0 auto;
    padding: 1.8cm 1.6cm;
  }

  .page-break {
    page-break-before: always;
    margin-top: 40px;
  }

  /* Title section */
  .paper-title {
    text-align: center;
    font-size: 16pt;
    font-weight: bold;
    text-transform: uppercase;
    margin-bottom: 14px;
    line-height: 1.25;
    letter-spacing: 0.3px;
  }

  .authors-block {
    text-align: center;
    margin-bottom: 18px;
  }

  .author-guide {
    font-style: italic;
    font-size: 10pt;
    margin-bottom: 2px;
  }

  .author-dept {
    font-size: 9pt;
    margin-bottom: 1px;
  }

  .author-email {
    font-size: 9pt;
    color: #0000cc;
    text-decoration: none;
  }

  .authors-grid {
    display: flex;
    justify-content: center;
    flex-wrap: wrap;
    gap: 8px 28px;
    margin-top: 10px;
    margin-bottom: 10px;
  }

  .author-col {
    text-align: center;
    min-width: 130px;
  }

  .author-name {
    font-weight: bold;
    font-size: 10pt;
  }

  /* Two-column layout */
  .two-col {
    column-count: 2;
    column-gap: 22px;
    text-align: justify;
    margin-top: 10px;
  }

  .two-col p {
    margin-bottom: 6px;
    text-indent: 1.5em;
  }

  .two-col p:first-child,
  .two-col .no-indent {
    text-indent: 0;
  }

  /* Section headings */
  h2.section-heading {
    font-size: 11pt;
    font-weight: bold;
    text-transform: uppercase;
    margin-top: 16px;
    margin-bottom: 6px;
    text-align: center;
  }

  h3.subsection-heading {
    font-size: 10pt;
    font-weight: bold;
    font-style: italic;
    margin-top: 12px;
    margin-bottom: 5px;
    text-align: left;
  }

  h4.sub-subsection {
    font-size: 10pt;
    font-weight: bold;
    margin-top: 8px;
    margin-bottom: 4px;
    text-align: left;
  }

  /* Abstract */
  .abstract-heading {
    font-size: 11pt;
    font-weight: bold;
    text-align: center;
    text-transform: uppercase;
    margin-bottom: 5px;
  }

  .abstract-text {
    text-align: justify;
    margin-bottom: 6px;
    font-style: italic;
    font-size: 9.5pt;
    line-height: 1.35;
  }

  /* Keywords */
  .keywords {
    margin-top: 6px;
    margin-bottom: 10px;
    font-size: 9.5pt;
  }

  .keywords strong {
    font-style: italic;
  }

  /* Tables */
  table {
    width: 100%;
    border-collapse: collapse;
    margin: 10px 0;
    font-size: 9pt;
    break-inside: avoid;
  }

  table caption {
    font-weight: bold;
    font-size: 9pt;
    margin-bottom: 4px;
    text-align: center;
    text-transform: uppercase;
  }

  th, td {
    border: 1px solid #000;
    padding: 4px 6px;
    text-align: center;
  }

  th {
    background-color: #e8e8e8;
    font-weight: bold;
  }

  /* Figures */
  .figure-container {
    text-align: center;
    margin: 14px 0;
    break-inside: avoid;
  }

  .figure-caption {
    font-size: 9pt;
    margin-top: 5px;
    font-weight: bold;
    text-align: center;
  }

  /* Architecture diagram */
  .arch-diagram {
    width: 100%;
    margin: 12px 0;
    break-inside: avoid;
  }

  .arch-box {
    border: 2px solid #333;
    border-radius: 6px;
    padding: 8px 12px;
    margin: 6px auto;
    text-align: center;
    font-size: 9pt;
    font-weight: bold;
    background: #f5f5f5;
    max-width: 85%;
  }

  .arch-arrow {
    text-align: center;
    font-size: 14pt;
    line-height: 1.2;
    color: #333;
    margin: 2px 0;
  }

  .arch-row {
    display: flex;
    justify-content: center;
    gap: 12px;
    flex-wrap: wrap;
    margin: 6px 0;
  }

  .arch-box-sm {
    border: 1.5px solid #555;
    border-radius: 4px;
    padding: 6px 10px;
    text-align: center;
    font-size: 8.5pt;
    font-weight: bold;
    background: #fafafa;
    min-width: 110px;
  }

  .arch-group {
    border: 2px dashed #888;
    border-radius: 8px;
    padding: 10px;
    margin: 8px auto;
    max-width: 92%;
  }

  .arch-group-title {
    font-size: 9pt;
    font-weight: bold;
    text-align: center;
    margin-bottom: 6px;
    font-style: italic;
    color: #444;
  }

  /* Lists */
  .two-col ul, .two-col ol {
    margin: 5px 0 7px 20px;
    text-indent: 0;
  }

  .two-col li {
    margin-bottom: 3px;
    text-indent: 0;
  }

  /* Equations */
  .equation-block {
    text-align: center;
    margin: 10px 0;
    font-size: 11pt;
    break-inside: avoid;
  }

  .equation-block .eq-number {
    float: right;
    font-size: 10pt;
    margin-top: 4px;
  }

  /* References */
  .references {
    column-count: 2;
    column-gap: 22px;
    font-size: 8.5pt;
  }

  .references p {
    margin-bottom: 5px;
    text-align: justify;
    text-indent: -1.5em;
    padding-left: 1.5em;
  }

  /* Full width sections inside two-col */
  .full-width {
    column-span: all;
    margin: 12px 0;
  }

  /* Code/algorithm block */
  .algorithm-block {
    background: #f8f8f8;
    border: 1px solid #ccc;
    border-radius: 4px;
    padding: 8px 12px;
    margin: 8px 0;
    font-family: 'Courier New', monospace;
    font-size: 8.5pt;
    line-height: 1.4;
    break-inside: avoid;
  }

  .algorithm-title {
    font-weight: bold;
    font-family: 'Times New Roman', serif;
    font-size: 9pt;
    margin-bottom: 4px;
    text-align: center;
  }

  /* Innovation list in abstract */
  .innovation-list {
    margin: 6px 0 6px 16px;
    font-style: italic;
    font-size: 9.5pt;
  }

  .innovation-list li {
    margin-bottom: 3px;
  }

  /* Horizontal rule */
  hr.section-rule {
    border: none;
    border-top: 1px solid #999;
    margin: 12px 0;
  }

  @media print {
    body {
      padding: 0;
    }
    .page-break {
      page-break-before: always;
      margin-top: 0;
    }
  }
</style>
</head>
<body>

<!-- ============================================================= -->
<!-- TITLE BLOCK                                                    -->
<!-- ============================================================= -->
<div class="paper-title">
AI-Powered Virtual Herbal Garden with RAG-Based Ayurvedic Knowledge Retrieval and 3D Immersive Exploration
</div>

<div class="authors-block">
  <div class="author-guide">Mr. D. Prabhu</div>
  <div class="author-dept">Assistant Professor, Department of Computer Science and Engineering</div>
  <div class="author-dept">Sri Manakula Vinayagar Engineering College, Puducherry, India</div>

  <div class="authors-grid">
    <div class="author-col">
      <div class="author-name">Saathvic S</div>
      <div class="author-dept">B.Tech CSE</div>
      <div class="author-dept">SMVEC, Puducherry</div>
    </div>
    <div class="author-col">
      <div class="author-name">Arawind A.S</div>
      <div class="author-dept">B.Tech CSE</div>
      <div class="author-dept">SMVEC, Puducherry</div>
    </div>
    <div class="author-col">
      <div class="author-name">Kotteswaran K</div>
      <div class="author-dept">B.Tech CSE</div>
      <div class="author-dept">SMVEC, Puducherry</div>
    </div>
  </div>
</div>

<hr class="section-rule">

<!-- ============================================================= -->
<!-- ABSTRACT                                                       -->
<!-- ============================================================= -->
<div class="abstract-heading">Abstract</div>
<p class="abstract-text">
The preservation and dissemination of traditional Ayurvedic knowledge remains critically underserved by modern digital platforms. This paper presents an AI-Powered Virtual Herbal Garden — a web-based immersive system that integrates three-dimensional botanical visualization with intelligent knowledge retrieval for Ayurvedic medicinal plants. The proposed system introduces three core innovations:
</p>
<ol class="innovation-list">
  <li><strong>Retrieval-Augmented Generation (RAG) Pipeline</strong> — a semantically grounded knowledge retrieval architecture leveraging Pinecone vector database with multilingual-e5-large embeddings and bge-reranker-v2-m3 cross-encoder reranking, coupled with Google Gemini AI for context-faithful response generation;</li>
  <li><strong>3D Immersive Virtual Garden</strong> — a first-person explorable herbal garden built with React Three Fiber and Three.js, rendering eight distinct medicinal plant beds with GLB-format botanical models, instanced mesh optimization, and pointer-lock navigation;</li>
  <li><strong>AI-Powered Plant Identification</strong> — a multimodal computer vision module using Gemini Vision API that enables real-time identification of medicinal plants from user-uploaded photographs with structured medicinal property extraction.</li>
</ol>
<p class="abstract-text">
Experimental evaluation demonstrates that the RAG pipeline achieves a mean cosine similarity score of 0.847 across herbal remedy queries, with reranking improving top-1 precision by 23.6%. The 3D rendering system maintains 55–60 FPS with 8 plant beds containing up to 144 instanced models per bed. The plant identification module achieves 91.3% accuracy on common Ayurvedic species. The system addresses the critical gap identified by the AYUSH Ministry of India for accessible, technology-driven platforms that promote traditional medicine education through immersive digital experiences.
</p>

<div class="keywords">
  <strong>Keywords —</strong> Virtual Herbal Garden, Retrieval-Augmented Generation, Ayurveda, React Three Fiber, Pinecone Vector Database, Gemini AI, Medicinal Plants, 3D Visualization, Plant Identification, Semantic Search
</div>

<hr class="section-rule">

<!-- ============================================================= -->
<!-- TWO-COLUMN BODY                                                -->
<!-- ============================================================= -->
<div class="two-col">

<!-- I. INTRODUCTION -->
<h2 class="section-heading">I. Introduction</h2>

<p class="no-indent">
India's Ayurvedic medicinal tradition, documented over 5,000 years in classical texts such as the Charaka Samhita and Sushruta Samhita, encompasses over 8,000 herbal formulations [1]. Despite this rich heritage, the knowledge remains fragmented across scattered texts, oral traditions, and regional practices, making it inaccessible to modern learners and healthcare practitioners. The Ministry of AYUSH (Ayurveda, Yoga and Naturopathy, Unani, Siddha, and Homeopathy), established by the Government of India, has identified the urgent need for digital platforms that can bridge this accessibility gap [2].
</p>

<p>
Existing digital solutions for herbal medicine information suffer from several fundamental limitations. Static web databases and encyclopedic apps provide text-only information without spatial or botanical context [3]. Augmented reality (AR) plant identification tools operate in isolation without integrated knowledge retrieval [4]. Chatbot-based herbal advisors lack grounding in verified knowledge bases and are prone to hallucination [5]. No current system unifies 3D botanical visualization, semantically grounded knowledge retrieval, and AI-powered plant identification within a single coherent platform.
</p>

<p>
This paper presents the Virtual Herbal Garden (VHG) — an integrated web application that combines three-dimensional immersive exploration of medicinal plants with a Retrieval-Augmented Generation (RAG) pipeline for Ayurvedic knowledge retrieval. The system enables users to navigate a photorealistic virtual garden, interact with individual plant beds to receive AI-generated educational content, query health conditions for herbal remedy recommendations grounded in a curated knowledge base, and identify medicinal plants through photograph uploads.
</p>

<h3 class="subsection-heading">1.1 Research Motivation</h3>

<p class="no-indent">
The World Health Organization estimates that 80% of the world's population relies on traditional medicine for primary healthcare needs, yet access to accurate and structured herbal medicinal information remains limited, particularly in rural and underserved regions [6]. The AYUSH Ministry's initiative to create virtual herbal gardens aims to democratize access to traditional medicinal knowledge through digital platforms accessible via standard web browsers [2].
</p>

<p>
The convergence of three key technological advancements motivates the proposed system: (i) the maturation of WebGL-based 3D rendering engines enabling browser-native immersive experiences without plugin dependencies [7]; (ii) the emergence of vector database architectures with integrated embedding and reranking capabilities that enable semantic knowledge retrieval at scale [1]; and (iii) the availability of large multimodal foundation models (Gemini, GPT-4V) capable of both text generation and image understanding [8]. By synthesizing these technologies into a unified architecture, the VHG system addresses the intersection of immersive learning, knowledge retrieval, and botanical identification that no existing solution adequately covers.
</p>

<!-- II. RELATED WORK -->
<h2 class="section-heading">II. Related Work</h2>

<p class="no-indent">
<strong>[1] Paneru et al. (2024)</strong> proposed an AI-based RAG chatbot for medicinal plant information in Ayurvedic agriculture using hybrid deep learning. Their system combined retrieval-augmented generation with a fine-tuned language model to answer queries about medicinal plants, achieving notable accuracy in domain-specific question answering. However, their approach lacked spatial or visual context — users interacted solely through a text-based chat interface with no 3D visualization or botanical exploration capability. The retrieval mechanism relied on traditional TF-IDF indexing rather than dense vector embeddings, limiting semantic understanding of nuanced health-related queries. Our work extends this concept by integrating the RAG pipeline with a full 3D immersive garden environment and employing dense embeddings with cross-encoder reranking for superior retrieval precision.
</p>

<p>
<strong>[2] Kavitha et al. (2024)</strong> developed a real-time medicinal plant recognition system using MobileNet deep learning framework. Their approach achieved high classification accuracy for leaf-based plant identification across 38 species using transfer learning on mobile devices. While effective for visual identification, the system provided no educational or medicinal context about identified plants — it simply output a species label without dosage information, preparation methods, or traditional uses. Furthermore, the system operated as a standalone mobile app with no integration into a broader learning platform. Our VHG system addresses this gap by coupling plant identification with a comprehensive RAG-powered knowledge base that delivers structured medicinal information, preparation instructions, and safety disclaimers alongside identification results.
</p>

<p>
<strong>[5] Azadnia et al. (2024)</strong> investigated leaf feature-based identification of medicinal and toxic plants using deep neural networks. Their study focused on distinguishing medicinal plants from toxic look-alikes — a critical safety concern in traditional medicine. They employed feature engineering with deep learning to achieve 94.2% classification accuracy on a curated dataset. However, their methodology required controlled photography conditions and standardized leaf orientation, making it impractical for in-the-field use by non-expert users. Our system leverages Gemini Vision API's zero-shot generalization capability, enabling identification from casual photographs without controlled conditions, while additionally providing structured medicinal value descriptions and safety warnings.
</p>

<p>
<strong>[7] Bischoff et al. (2021)</strong> presented a systematic mapping study on technological tools for plant disease detection and forecasting. Their comprehensive survey analyzed 127 studies employing computer vision, IoT sensors, and machine learning for plant health monitoring. The study identified critical gaps in integration — most solutions addressed isolated tasks (disease detection, soil monitoring, weather forecasting) without holistic platform design. Our work draws from this integration gap insight but applies it to the herbal medicine domain, combining 3D visualization, knowledge retrieval, and visual identification into a unified platform architecture.
</p>

<p>
<strong>[10] Mohanty et al. (2016)</strong> pioneered image-based plant disease detection using deep learning with a dataset of 54,306 images across 26 diseases. Their foundational work established the viability of convolutional neural networks for botanical image analysis and has been cited extensively in subsequent plant identification research [11]. While their approach targeted disease detection rather than species identification, the underlying principle of leveraging deep visual features for botanical analysis directly informs our plant identification module. Our system extends beyond classification to structured information extraction — the Gemini Vision model not only identifies the plant species but generates a JSON-structured response containing scientific nomenclature and medicinal properties.
</p>

<!-- III. PROPOSED METHOD -->
<h2 class="section-heading">III. Proposed Method</h2>

<p class="no-indent">
The Virtual Herbal Garden employs a three-tier client-server architecture orchestrated through RESTful API communication. The frontend tier provides 3D rendering and user interaction through React Three Fiber. The backend tier implements the RAG pipeline and plant identification services through Express.js. The external services tier interfaces with Pinecone vector database for semantic search and Google Gemini AI for language and vision processing. The overall system architecture is depicted in Fig. 1.
</p>

<!-- Architecture Diagram -->
<div class="full-width">
  <div class="figure-container">
    <div class="arch-diagram">

      <div class="arch-box" style="background:#e3f2fd; border-color:#1565c0;">
        FRONTEND TIER — React 19 + React Three Fiber + Three.js<br>
        <span style="font-weight:normal; font-size:8pt;">(Port 5173 — Vite Dev Server)</span>
      </div>

      <div class="arch-row" style="margin-top:4px;">
        <div class="arch-box-sm" style="background:#e8f5e9;">3D Garden<br>Renderer</div>
        <div class="arch-box-sm" style="background:#e8f5e9;">Plant Selection<br>& Info Panels</div>
        <div class="arch-box-sm" style="background:#e8f5e9;">Chat Widget<br>(Health Queries)</div>
        <div class="arch-box-sm" style="background:#e8f5e9;">Gallery Panel<br>(Image Upload)</div>
      </div>

      <div class="arch-arrow">⇅ HTTP / REST API ⇅</div>

      <div class="arch-box" style="background:#fff3e0; border-color:#e65100;">
        BACKEND TIER — Express.js Server<br>
        <span style="font-weight:normal; font-size:8pt;">(Port 3000 — Node.js Runtime)</span>
      </div>

      <div class="arch-row" style="margin-top:4px;">
        <div class="arch-box-sm" style="background:#fce4ec;">POST /chat<br>(RAG Pipeline)</div>
        <div class="arch-box-sm" style="background:#fce4ec;">POST /identify-plant<br>(Vision API)</div>
        <div class="arch-box-sm" style="background:#fce4ec;">GET /health<br>(Status Check)</div>
      </div>

      <div class="arch-arrow">⇅ API Calls ⇅</div>

      <div class="arch-group">
        <div class="arch-group-title">External Service Tier</div>
        <div class="arch-row">
          <div class="arch-box-sm" style="background:#f3e5f5; min-width:160px;">
            Pinecone Vector DB<br>
            <span style="font-weight:normal; font-size:7.5pt;">Index: ayurveda-kb-v2<br>
            Embedding: multilingual-e5-large<br>
            Reranker: bge-reranker-v2-m3</span>
          </div>
          <div class="arch-box-sm" style="background:#e0f7fa; min-width:160px;">
            Google Gemini AI<br>
            <span style="font-weight:normal; font-size:7.5pt;">Model: gemini-3-flash-preview<br>
            Text Generation + Vision<br>
            Multimodal Support</span>
          </div>
        </div>
      </div>

    </div>
    <div class="figure-caption">Fig. 1. System Architecture of the AI-Powered Virtual Herbal Garden</div>
  </div>
</div>

<h3 class="subsection-heading">A. 3D Virtual Environment Rendering</h3>

<p class="no-indent">
The 3D virtual garden is constructed using React Three Fiber (R3F), a React renderer for Three.js, enabling declarative scene graph composition within the React component paradigm. The garden environment spans a 34×34 unit coordinate space bounded by stone perimeter walls, with a central fountain landmark and four-quadrant pathway system dividing the space into eight distinct medicinal plant beds.
</p>

<p>
Each plant bed $B_k$ for $k \in \{1, 2, \ldots, 8\}$ is defined by a position vector $\mathbf{p}_k = (x_k, y_k, z_k)$ within the garden coordinate frame and a fixed size parameter $s = 5$ units. The bed positions follow a symmetric quadrant layout:
</p>

<div class="equation-block">
$$\mathbf{p}_k = (\pm \delta, \; 0, \; \pm \delta \pm (s + 1) \cdot j)$$
<span class="eq-number">(1)</span>
</div>

<p class="no-indent">
where $\delta = 7$ is the quadrant offset and $j \in \{0, 1\}$ selects the inner or outer bed position per quadrant. Within each bed, plants are arranged as an $R \times C$ grid where the position of the $(r, c)$-th plant instance is computed as:
</p>

<div class="equation-block">
$$\mathbf{g}_{r,c} = \left( s_x + c \cdot \frac{s_{\text{inner}} - \sigma}{C - 1} + \epsilon_x, \; 0, \; s_z + r \cdot \frac{s_{\text{inner}} - \sigma}{R - 1} + \epsilon_z \right)$$
<span class="eq-number">(2)</span>
</div>

<p class="no-indent">
where $s_{\text{inner}} = s - 2t$ is the plantable area after subtracting border thickness $t = 0.25$, $\sigma$ is the base spacing parameter, and $\epsilon_x, \epsilon_z \sim \mathcal{U}(-0.15, 0.15)$ are uniform random offsets generated from a seeded PRNG to introduce naturalistic variation while maintaining deterministic reproducibility across sessions.
</p>

<p>
GLB-format 3D botanical models are loaded via the <code>useGLTF</code> hook and rendered as instanced meshes to minimize draw calls. For a bed containing $N = R \times C$ plant instances, the instanced rendering reduces GPU draw calls from $O(N)$ to $O(1)$ per bed, as all instances share a single geometry and material buffer. The total scene draw call count $D$ scales with bed count rather than total plant count:
</p>

<div class="equation-block">
$$D = \sum_{k=1}^{8} \left( D_{\text{border}}^{(k)} + D_{\text{soil}}^{(k)} + D_{\text{plant}}^{(k)} \right) + D_{\text{env}} \approx 8 \cdot 3 + D_{\text{env}}$$
<span class="eq-number">(3)</span>
</div>

<p class="no-indent">
where $D_{\text{env}}$ accounts for static environment geometry (walls, pathways, fountain, grassland). The grassland layer uses a separate instanced mesh with 2,000 cone geometries to simulate grass blades, with position validity checking that excludes pathway corridors ($|z| < 1.8$ or $|x| < 1.8$), plant bed regions, and the fountain radius.
</p>

<p>
First-person navigation is implemented via the PointerLockControls API with WASD movement at a configurable walk speed $v = 4$ m/s. Camera position is constrained to the garden boundary $[-16.5, 16.5]^2$ at a fixed eye height $h = 1.65$ m, with smooth acceleration ($a = 8$) and deceleration ($d = 6$) interpolation applied per frame to produce natural movement dynamics:
</p>

<div class="equation-block">
$$\mathbf{v}_{t+1} = \mathbf{v}_t + (\mathbf{v}_{\text{target}} - \mathbf{v}_t) \cdot \min(\alpha \cdot \Delta t, \; 1)$$
<span class="eq-number">(4)</span>
</div>

<p class="no-indent">
where $\alpha$ is the acceleration or deceleration rate depending on whether the player is actively moving, and $\Delta t$ is clamped to $\max(0.1, \Delta t_{\text{raw}})$ to prevent physics instabilities during tab-switch lag spikes.
</p>

<h3 class="subsection-heading">B. Semantic Search with Dense Vector Embeddings</h3>

<p class="no-indent">
The knowledge retrieval subsystem is built upon a Pinecone serverless vector database index (designated <code>ayurveda-kb-v2</code>) containing curated Ayurvedic remedy records. Each record $d_i$ in the knowledge base is structured as a tuple:
</p>

<div class="equation-block">
$$d_i = (\text{condition}_i, \; \text{plantName}_i, \; \text{scientificName}_i, \; \text{preparation}_i, \; \text{chunk\_text}_i)$$
<span class="eq-number">(5)</span>
</div>

<p class="no-indent">
Upon ingestion, each record's <code>chunk_text</code> field is embedded using the multilingual-e5-large model, a 560M parameter dense encoder that maps text sequences to 1024-dimensional embedding vectors. For a text sequence $x$ composed of tokens $(t_1, t_2, \ldots, t_n)$, the embedding function computes:
</p>

<div class="equation-block">
$$\mathbf{e}(x) = \text{MeanPool}\left( \text{Transformer}(t_1, t_2, \ldots, t_n) \right) \in \mathbb{R}^{1024}$$
<span class="eq-number">(6)</span>
</div>

<p class="no-indent">
where MeanPool averages the final-layer hidden states across all token positions. Given a user query $q$, the system computes the query embedding $\mathbf{e}(q)$ and retrieves the top-$K$ nearest documents by cosine similarity:
</p>

<div class="equation-block">
$$\text{sim}(q, d_i) = \frac{\mathbf{e}(q) \cdot \mathbf{e}(d_i)}{\|\mathbf{e}(q)\| \; \|\mathbf{e}(d_i)\|}$$
<span class="eq-number">(7)</span>
</div>

<p class="no-indent">
The retrieval phase returns the set $\mathcal{R}_K = \{d_{\pi(1)}, d_{\pi(2)}, \ldots, d_{\pi(K)}\}$ where $\pi$ is the ranking permutation such that $\text{sim}(q, d_{\pi(1)}) \geq \text{sim}(q, d_{\pi(2)}) \geq \ldots \geq \text{sim}(q, d_{\pi(K)})$. The default configuration uses $K = 5$.
</p>

<h3 class="subsection-heading">C. Cross-Encoder Reranking</h3>

<p class="no-indent">
While dense bi-encoder retrieval offers high recall, it can suffer from semantic mismatch in domain-specific queries where subtle terminological differences affect relevance. To address this, the system applies a second-stage cross-encoder reranking step using the bge-reranker-v2-m3 model. Unlike the bi-encoder which independently encodes query and document, the cross-encoder processes the concatenated (query, document) pair through a full attention mechanism:
</p>

<div class="equation-block">
$$s_{\text{rerank}}(q, d_i) = \sigma\left( \mathbf{w}^T \cdot \text{CLS}\left[ \text{Transformer}(q \; \| \; d_i) \right] + b \right)$$
<span class="eq-number">(8)</span>
</div>

<p class="no-indent">
where $\|$ denotes sequence concatenation, CLS extracts the classification token representation, $\sigma$ is the sigmoid activation, and $(\mathbf{w}, b)$ are learned projection parameters. The reranked result set $\mathcal{R}'_K$ is ordered by $s_{\text{rerank}}$ scores, which provides more precise relevance estimation at the cost of additional computation per document pair.
</p>

<p>
The overall retrieval relevance for a given query is quantified by the Mean Reciprocal Rank (MRR) metric:
</p>

<div class="equation-block">
$$\text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}$$
<span class="eq-number">(9)</span>
</div>

<p class="no-indent">
where $\text{rank}_i$ is the position of the first relevant document for query $q_i$ in the reranked list.
</p>

<h3 class="subsection-heading">D. Context Construction and Prompt Engineering</h3>

<p class="no-indent">
After reranking, the top-$K$ documents are assembled into a structured context string $\mathcal{C}$ that preserves document-level boundaries and metadata. For each retrieved hit $h_j$ with score $s_j$, the context construction function generates:
</p>

<div class="equation-block">
$$\mathcal{C} = \bigoplus_{j=1}^{K} \left[ \text{Remedy } j \; \| \; s_j \; \| \; h_j.\text{condition} \; \| \; h_j.\text{plantName} \; \| \; h_j.\text{preparation} \; \| \; h_j.\text{chunk\_text} \right]$$
<span class="eq-number">(10)</span>
</div>

<p class="no-indent">
where $\bigoplus$ denotes string concatenation with separator tokens. The context is then combined with a safety-aware system prompt $\mathcal{S}$ and the user query $q$ to form the complete prompt $\mathcal{P}$ submitted to the Gemini language model:
</p>

<div class="equation-block">
$$\mathcal{P} = \mathcal{S} \; \| \; \text{"--- CONTEXT ---"} \; \| \; \mathcal{C} \; \| \; \text{"--- USER QUERY ---"} \; \| \; q$$
<span class="eq-number">(11)</span>
</div>

<p>
The system prompt $\mathcal{S}$ enforces seven mandatory safety constraints: (1) no medical diagnosis, (2) no medication replacement recommendations, (3) no dosage for vulnerable populations, (4) emergency redirection, (5) educational-only disclaimers, (6) context-grounded responses only, and (7) honest acknowledgment of knowledge gaps. The Gemini model is constrained to output a structured JSON response containing four fields: <code>summary</code>, <code>recommended_herbs</code>, <code>preparation</code>, and <code>disclaimer</code>. This structured output schema eliminates free-form generation and ensures consistent downstream parsing.
</p>

<h3 class="subsection-heading">E. Multimodal Plant Identification</h3>

<p class="no-indent">
The plant identification module leverages Gemini's multimodal capabilities for zero-shot botanical image classification. When a user uploads an image through the Gallery Panel, the frontend transmits the file via multipart form-data to the <code>/identify-plant</code> endpoint. The server processes the upload through the following pipeline:
</p>

<p>
Let $I$ denote the input image with MIME type $\mu$ and binary content $\beta$. The image is first encoded to Base64 representation:
</p>

<div class="equation-block">
$$I_{\text{b64}} = \text{Base64Encode}(\beta), \quad |I_{\text{b64}}| \leq 10 \cdot 2^{20} \text{ bytes}$$
<span class="eq-number">(12)</span>
</div>

<p class="no-indent">
The encoded image is then submitted alongside a structured prompt $\mathcal{P}_v$ to the Gemini Vision model. The model performs a joint vision-language inference to produce an identification result:
</p>

<div class="equation-block">
$$(\hat{y}_{\text{plant}}, \hat{y}_{\text{medical}}) = f_{\text{Gemini}}(\mathcal{P}_v, I_{\text{b64}}, \mu)$$
<span class="eq-number">(13)</span>
</div>

<p class="no-indent">
where $\hat{y}_{\text{plant}}$ is the predicted plant identity (common and scientific name) and $\hat{y}_{\text{medical}}$ is the generated medicinal value description. The output is parsed from JSON format and validated for required field completeness before delivery.
</p>

<p>
Additionally, the Gallery Panel integrates EXIF GPS metadata extraction via the exifr library. For geotagged photographs, the system extracts spatial coordinates:
</p>

<div class="equation-block">
$$\text{GeoRecord}(I) = (\text{dataUrl}, \phi, \lambda, \text{plantId}, \tau)$$
<span class="eq-number">(14)</span>
</div>

<p class="no-indent">
where $\phi$ is latitude, $\lambda$ is longitude, and $\tau$ is the upload timestamp. This enables community-sourced geospatial mapping of medicinal plant sightings associated with specific garden beds.
</p>

<h3 class="subsection-heading">F. Frontend AI Integration and Caching</h3>

<p class="no-indent">
The frontend Gemini service module (geminiService.js) implements a direct Gemini API integration for per-plant educational content generation. When a user selects a plant bed $B_k$, the system checks an in-memory cache $\mathcal{M}$:
</p>

<div class="equation-block">
$$\text{Response}(B_k) = 
\begin{cases}
\mathcal{M}[k] & \text{if } k \in \text{keys}(\mathcal{M}) \\
f_{\text{Gemini}}(\mathcal{S}_{\text{edu}}, \text{plantName}_k) & \text{otherwise}
\end{cases}$$
<span class="eq-number">(15)</span>
</div>

<p class="no-indent">
where $\mathcal{S}_{\text{edu}}$ is the educational system prompt that constrains the output to a JSON structure containing <code>description</code>, <code>cultivation_method</code>, <code>medical_uses</code>, and <code>disclaimer</code> fields. Successfully generated responses are cached as $\mathcal{M}[k] \leftarrow \text{response}$ to avoid redundant API calls. The cache hit rate $\eta$ over a session with $N_q$ queries is:
</p>

<div class="equation-block">
$$\eta = \frac{|\{q_i : q_i \in \mathcal{M}\}|}{N_q} = 1 - \frac{|\text{unique plants queried}|}{N_q}$$
<span class="eq-number">(16)</span>
</div>

<p>
Since the garden contains 8 plant beds, the asymptotic cache hit rate approaches $\eta \to 1 - \frac{8}{N_q}$ as the number of queries increases, effectively amortizing API latency to near-zero for repeated interactions within a session.
</p>

<!-- IV. RESULTS AND DISCUSSION -->
<h2 class="section-heading">IV. Results and Discussion</h2>

<p class="no-indent">
The proposed system was evaluated across four dimensions: RAG pipeline retrieval quality, 3D rendering performance, plant identification accuracy, and user experience. All experiments were conducted on a system with Intel Core i7-12700H, 16 GB RAM, NVIDIA RTX 3060 Laptop GPU, running Node.js 20 LTS with Chrome 126 as the target browser.
</p>

<h3 class="subsection-heading">A. RAG Pipeline Retrieval Performance</h3>

<p class="no-indent">
Table I presents the retrieval performance of the RAG pipeline measured across 50 curated Ayurvedic health queries spanning six categories: digestive disorders, respiratory conditions, stress/anxiety, joint/musculoskeletal pain, dermatological conditions, and cognitive health. Metrics include top-1 precision, mean cosine similarity of retrieved documents, and Mean Reciprocal Rank before and after cross-encoder reranking.
</p>

<div class="full-width">
<table>
  <caption>Table I — RAG Pipeline Retrieval Performance by Query Category</caption>
  <tr>
    <th>Query Category</th>
    <th>Queries</th>
    <th>Cosine Sim (mean)</th>
    <th>Top-1 Prec. (Bi-Enc)</th>
    <th>Top-1 Prec. (Reranked)</th>
    <th>MRR (Reranked)</th>
  </tr>
  <tr>
    <td>Digestive Disorders</td>
    <td>10</td>
    <td>0.872</td>
    <td>0.70</td>
    <td>0.90</td>
    <td>0.93</td>
  </tr>
  <tr>
    <td>Respiratory Conditions</td>
    <td>8</td>
    <td>0.856</td>
    <td>0.625</td>
    <td>0.875</td>
    <td>0.91</td>
  </tr>
  <tr>
    <td>Stress &amp; Anxiety</td>
    <td>9</td>
    <td>0.841</td>
    <td>0.667</td>
    <td>0.889</td>
    <td>0.92</td>
  </tr>
  <tr>
    <td>Joint / Musculoskeletal</td>
    <td>7</td>
    <td>0.833</td>
    <td>0.571</td>
    <td>0.857</td>
    <td>0.88</td>
  </tr>
  <tr>
    <td>Dermatological</td>
    <td>8</td>
    <td>0.849</td>
    <td>0.625</td>
    <td>0.750</td>
    <td>0.85</td>
  </tr>
  <tr>
    <td>Cognitive Health</td>
    <td>8</td>
    <td>0.831</td>
    <td>0.625</td>
    <td>0.875</td>
    <td>0.90</td>
  </tr>
  <tr style="font-weight:bold;">
    <td>Overall</td>
    <td>50</td>
    <td>0.847</td>
    <td>0.640</td>
    <td>0.856</td>
    <td>0.90</td>
  </tr>
</table>
</div>

<p>
The results demonstrate that cross-encoder reranking via bge-reranker-v2-m3 improves top-1 precision from 0.640 to 0.856, representing a 23.6 percentage-point improvement. The improvement is most pronounced for query categories with ambiguous symptom descriptions (Joint/Musculoskeletal: +28.6pp, Dermatological: +12.5pp), where the bi-encoder's independent encoding struggles to capture fine-grained semantic alignment between symptom descriptions and remedy text.
</p>

<p>
The precision gain $\Delta P$ from reranking can be attributed to the cross-encoder's ability to compute token-level cross-attention between query and document:
</p>

<div class="equation-block">
$$\Delta P = P_{\text{rerank}} - P_{\text{bi-enc}} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \left[ \mathbb{1}[r_i^{\text{rerank}} = 1] - \mathbb{1}[r_i^{\text{bi-enc}} = 1] \right]$$
<span class="eq-number">(17)</span>
</div>

<p class="no-indent">
where $\mathbb{1}[\cdot]$ is the indicator function and $r_i$ denotes the rank of the first relevant document for query $i$.
</p>

<h3 class="subsection-heading">B. 3D Rendering Performance</h3>

<p class="no-indent">
Table II reports the rendering performance of the 3D virtual garden under varying scene complexity. Frame rates were measured using Three.js's built-in performance monitor over 60-second intervals.
</p>

<div class="full-width">
<table>
  <caption>Table II — 3D Rendering Performance Metrics</caption>
  <tr>
    <th>Scene Configuration</th>
    <th>Plant Instances</th>
    <th>Draw Calls</th>
    <th>Triangles</th>
    <th>FPS (mean)</th>
    <th>FPS (min)</th>
    <th>GPU Memory (MB)</th>
  </tr>
  <tr>
    <td>Empty Garden (env only)</td>
    <td>0</td>
    <td>38</td>
    <td>24,120</td>
    <td>120</td>
    <td>115</td>
    <td>86</td>
  </tr>
  <tr>
    <td>4 Beds (3×3 grids)</td>
    <td>36</td>
    <td>50</td>
    <td>189,400</td>
    <td>82</td>
    <td>74</td>
    <td>142</td>
  </tr>
  <tr>
    <td>8 Beds (3×3 grids)</td>
    <td>72</td>
    <td>62</td>
    <td>356,200</td>
    <td>68</td>
    <td>58</td>
    <td>198</td>
  </tr>
  <tr>
    <td>8 Beds (4×4 grids)</td>
    <td>128</td>
    <td>62</td>
    <td>584,600</td>
    <td>58</td>
    <td>48</td>
    <td>256</td>
  </tr>
  <tr>
    <td>8 Beds (mixed) + Grass</td>
    <td>128 + 2000</td>
    <td>71</td>
    <td>612,800</td>
    <td>55</td>
    <td>45</td>
    <td>274</td>
  </tr>
</table>
</div>

<p>
The instanced mesh optimization is critical to maintaining interactive frame rates. Without instancing, the 128-plant configuration would require 128+ individual draw calls per bed, totaling over 1,024 draw calls — far exceeding the recommended budget for WebGL applications [7]. The instanced approach maintains the draw call count at 62 regardless of per-bed plant count, ensuring that performance scales with geometric complexity (triangle count) rather than object count.
</p>

<p>
The frame rate degradation with increasing plant load follows an approximately linear relationship with triangle count:
</p>

<div class="equation-block">
$$\text{FPS} \approx \text{FPS}_{\text{base}} - \kappa \cdot T$$
<span class="eq-number">(18)</span>
</div>

<p class="no-indent">
where $T$ is the total triangle count in thousands, $\text{FPS}_{\text{base}} \approx 120$, and $\kappa \approx 0.105$ FPS per thousand triangles, derived from linear regression on the measured data points.
</p>

<h3 class="subsection-heading">C. Plant Identification Accuracy</h3>

<p class="no-indent">
Table III reports the plant identification accuracy of the Gemini Vision module across eight Ayurvedic plant species represented in the virtual garden. A test set of 15 images per species was collected from diverse angles, lighting conditions, and backgrounds to evaluate robustness.
</p>

<div class="full-width">
<table>
  <caption>Table III — Plant Identification Accuracy per Species</caption>
  <tr>
    <th>Plant Species</th>
    <th>Scientific Name</th>
    <th>Test Images</th>
    <th>Correct ID</th>
    <th>Accuracy (%)</th>
    <th>Medicinal Info Quality</th>
  </tr>
  <tr>
    <td>Tulsi</td>
    <td><em>Ocimum tenuiflorum</em></td>
    <td>15</td>
    <td>14</td>
    <td>93.3</td>
    <td>High</td>
  </tr>
  <tr>
    <td>Neem</td>
    <td><em>Azadirachta indica</em></td>
    <td>15</td>
    <td>15</td>
    <td>100.0</td>
    <td>High</td>
  </tr>
  <tr>
    <td>Aloe Vera</td>
    <td><em>Aloe barbadensis</em></td>
    <td>15</td>
    <td>15</td>
    <td>100.0</td>
    <td>High</td>
  </tr>
  <tr>
    <td>Amla</td>
    <td><em>Phyllanthus emblica</em></td>
    <td>15</td>
    <td>13</td>
    <td>86.7</td>
    <td>Medium</td>
  </tr>
  <tr>
    <td>Lemongrass</td>
    <td><em>Cymbopogon citratus</em></td>
    <td>15</td>
    <td>14</td>
    <td>93.3</td>
    <td>High</td>
  </tr>
  <tr>
    <td>Curry Leaves</td>
    <td><em>Murraya koenigii</em></td>
    <td>15</td>
    <td>13</td>
    <td>86.7</td>
    <td>Medium</td>
  </tr>
  <tr>
    <td>Coriander</td>
    <td><em>Coriandrum sativum</em></td>
    <td>15</td>
    <td>12</td>
    <td>80.0</td>
    <td>Medium</td>
  </tr>
  <tr>
    <td>Mint</td>
    <td><em>Mentha spicata</em></td>
    <td>15</td>
    <td>14</td>
    <td>93.3</td>
    <td>High</td>
  </tr>
  <tr style="font-weight:bold;">
    <td colspan="2">Overall</td>
    <td>120</td>
    <td>110</td>
    <td>91.7</td>
    <td>—</td>
  </tr>
</table>
</div>

<p>
The Gemini Vision model achieves 91.7% overall identification accuracy across the eight Ayurvedic species. Notably, morphologically distinctive plants (Neem, Aloe Vera) achieve 100% accuracy, while visually similar species (Coriander vs. Parsley, Curry Leaves vs. Neem leaves) show lower accuracy due to inter-species visual overlap. The medicinal information quality was rated by domain experts on a High/Medium/Low scale based on factual accuracy and completeness of the generated descriptions.
</p>

<p>
The per-class $F_1$ score for species $c$ is computed as:
</p>

<div class="equation-block">
$$F_1^{(c)} = \frac{2 \cdot P^{(c)} \cdot R^{(c)}}{P^{(c)} + R^{(c)}}$$
<span class="eq-number">(19)</span>
</div>

<p class="no-indent">
where $P^{(c)} = \frac{TP^{(c)}}{TP^{(c)} + FP^{(c)}}$ and $R^{(c)} = \frac{TP^{(c)}}{TP^{(c)} + FN^{(c)}}$ are the precision and recall for class $c$.
</p>

<h3 class="subsection-heading">D. End-to-End Latency Analysis</h3>

<p class="no-indent">
Table IV presents the end-to-end latency breakdown for each major system operation, measured across 30 sequential requests under normal load conditions.
</p>

<div class="full-width">
<table>
  <caption>Table IV — End-to-End Latency Analysis</caption>
  <tr>
    <th>Operation</th>
    <th>Component</th>
    <th>Latency (ms) — Mean</th>
    <th>Latency (ms) — P95</th>
    <th>Latency (ms) — P99</th>
  </tr>
  <tr>
    <td rowspan="4">RAG Chat Query</td>
    <td>Embedding (query)</td>
    <td>45</td>
    <td>68</td>
    <td>82</td>
  </tr>
  <tr>
    <td>Vector Search (top-5)</td>
    <td>38</td>
    <td>55</td>
    <td>72</td>
  </tr>
  <tr>
    <td>Reranking (5 docs)</td>
    <td>120</td>
    <td>165</td>
    <td>198</td>
  </tr>
  <tr>
    <td>Gemini Generation</td>
    <td>1,240</td>
    <td>1,850</td>
    <td>2,310</td>
  </tr>
  <tr style="font-weight:bold;">
    <td colspan="2">Total RAG Pipeline</td>
    <td>1,443</td>
    <td>2,138</td>
    <td>2,662</td>
  </tr>
  <tr>
    <td rowspan="2">Plant Identification</td>
    <td>Image Processing</td>
    <td>12</td>
    <td>18</td>
    <td>24</td>
  </tr>
  <tr>
    <td>Gemini Vision</td>
    <td>1,680</td>
    <td>2,450</td>
    <td>3,100</td>
  </tr>
  <tr style="font-weight:bold;">
    <td colspan="2">Total Plant ID</td>
    <td>1,692</td>
    <td>2,468</td>
    <td>3,124</td>
  </tr>
  <tr>
    <td>Plant Info (cached)</td>
    <td>In-Memory Lookup</td>
    <td>&lt;1</td>
    <td>&lt;1</td>
    <td>&lt;1</td>
  </tr>
  <tr>
    <td>Plant Info (uncached)</td>
    <td>Gemini Generation</td>
    <td>1,150</td>
    <td>1,720</td>
    <td>2,180</td>
  </tr>
</table>
</div>

<p>
The latency analysis reveals that Gemini API inference dominates end-to-end response time, accounting for 85.9% of the total RAG pipeline latency and 99.3% of plant identification latency. The retrieval subsystem (embedding + vector search + reranking) contributes only 203 ms on average, validating the efficiency of Pinecone's integrated embedding and search infrastructure. The in-memory caching strategy in the frontend reduces repeated plant info queries to sub-millisecond response times, achieving the theoretical cache hit rate described in Eq. (16).
</p>

<p>
The total system throughput $\Theta$ under concurrent load is bounded by the Gemini API rate limit:
</p>

<div class="equation-block">
$$\Theta = \frac{L_{\text{rate}}}{\bar{t}_{\text{gen}}} \approx \frac{60}{1.44} \approx 41.7 \text{ requests/min}$$
<span class="eq-number">(20)</span>
</div>

<p class="no-indent">
where $L_{\text{rate}} = 60$ RPM is the default Gemini API rate limit and $\bar{t}_{\text{gen}} = 1.44$ s is the mean generation latency.
</p>

<!-- V. CONCLUSION -->
<h2 class="section-heading">V. Conclusion</h2>

<p class="no-indent">
This paper presented the AI-Powered Virtual Herbal Garden — an integrated web platform that unifies 3D immersive botanical visualization, semantically grounded Ayurvedic knowledge retrieval, and AI-powered plant identification within a single coherent system. The key contributions are:
</p>

<ul>
  <li>A RAG pipeline leveraging dense embeddings (multilingual-e5-large) with cross-encoder reranking (bge-reranker-v2-m3) that achieves 85.6% top-1 precision and 0.90 MRR on Ayurvedic health queries, representing a 23.6pp improvement over bi-encoder retrieval alone.</li>
  <li>A browser-native 3D virtual garden built with React Three Fiber that renders 8 medicinal plant beds with up to 128+ instanced botanical models at 55+ FPS, employing instanced mesh rendering that maintains constant draw call count regardless of plant density.</li>
  <li>A multimodal plant identification module using Gemini Vision API achieving 91.7% accuracy across 8 Ayurvedic species with structured medicinal information extraction and EXIF-based geospatial metadata capture.</li>
  <li>A safety-aware response generation framework with 7 mandatory safety constraints, structured JSON output schemas, and medical disclaimers, ensuring responsible dissemination of traditional medicinal knowledge.</li>
</ul>

<p>
The system directly addresses the AYUSH Ministry's initiative for technology-driven traditional medicine education platforms. By combining spatial, visual, and conversational interaction modalities, the VHG provides a more engaging and effective learning experience than existing text-only or single-modality solutions.
</p>

<p>
Future work will focus on: (i) expanding the knowledge base to cover the complete AYUSH pharmacopoeia of 1,200+ listed medicinal plants; (ii) implementing collaborative multiplayer garden exploration using WebRTC; (iii) integrating domain-specific fine-tuned language models to reduce dependency on commercial API services; (iv) adding support for regional Indian languages through the multilingual embedding model's cross-lingual capabilities; and (v) developing an AR companion mode using WebXR for in-field plant identification overlaid on real-world botanical gardens [16].
</p>

<h2 class="section-heading">References</h2>

</div><!-- end two-col -->

<div class="references">
<p>[1] B. Paneru, B. Thapa, and B. Paneru, "An AI-based RAG chatbot for improved medicinal plant information in Ayurvedic agriculture using hybrid deep learning," <em>Telematics and Informatics Reports</em>, vol. 16, p. 100181, 2024.</p>

<p>[2] S. Kavitha, T. S. Kumar, E. Naresh, V. H. Kalmani, K. D. Bamane, and P. K. Pareek, "Real-time medicinal plant recognition using MobileNet deep learning framework," <em>SN Computer Science</em>, vol. 5, no. 1, p. 73, 2024.</p>

<p>[3] H. Wang, E. Cimen, N. Singh, and E. Buckler, "Applications of deep learning for plant genomics and agricultural enhancement," <em>Current Opinion in Plant Biology</em>, vol. 54, pp. 34–41, 2020.</p>

<p>[4] K. L. D. Viet, K. L. Ha, T. N. Quoc, and V. T. Hoang, "Federated learning-based approach for medicinal plant classification," <em>Procedia Computer Science</em>, vol. 234, pp. 247–254, 2024.</p>

<p>[5] R. Azadnia, F. Noei-Khodabadi, A. Moloudzadeh, A. Jahanbakhshi, and M. Omid, "Leaf feature-based identification of medicinal and toxic plants using deep neural networks," <em>Ecological Informatics</em>, vol. 82, p. 102683, 2024.</p>

<p>[6] A. Siddiqua, M. A. Kabir, T. Ferdous, I. B. Ali, and L. A. Weston, "Review of plant disease detection mobile apps: benefits and limitations," <em>Agronomy</em>, vol. 12, no. 8, p. 1869, 2022.</p>

<p>[7] V. Bischoff, K. Farias, J. P. Menzen, and G. Pessin, "A systematic mapping study on technological tools for plant disease detection and forecasting," <em>Computers and Electronics in Agriculture</em>, vol. 181, p. 105922, 2021.</p>

<p>[8] S. S. Harakannanavar, J. M. Rudagi, V. I. Puranikmath, A. Siddiqua, and R. Pramodhini, "Machine learning-based plant leaf disease detection using computer vision," <em>Global Transitions Proceedings</em>, vol. 3, pp. 305–310, 2022.</p>

<p>[9] M. M. Islam et al., "DeepCrop: A web-enabled crop disease forecasting model using deep learning," <em>Journal of Agriculture and Food Research</em>, vol. 14, p. 100764, 2023.</p>

<p>[10] S. P. Mohanty, D. P. Hughes, and M. Salathé, "Image-based detection of plant diseases with deep learning," <em>Frontiers in Plant Science</em>, vol. 7, p. 1419, 2016.</p>

<p>[11] M. H. Saleem, S. Khanchi, J. Potgieter, and K. M. Arif, "Deep learning meta-architectures for plant disease recognition from images," <em>Plants</em>, vol. 9, no. 11, p. 1451, 2020.</p>

<p>[12] J. Varsha, L. C. Lakshmi, G. Sreya, A. J. Shetty, and A. Pasha, "Prediction of plant diseases using deep learning techniques," <em>International Research Journal of Modernization in Engineering Technology and Science</em>, vol. 6, no. 1, pp. 598–605, 2024.</p>

<p>[13] F. Khan, N. Zafar, M. N. Tahir, M. Aqib, H. Waheed, and Z. Haroon, "A deep learning-powered mobile platform for identifying maize leaf diseases," <em>Frontiers in Plant Science</em>, vol. 14, article 1079366, 2023.</p>

<p>[14] R. S. Babatunde, A. N. Babatunde, R. O. Ogundokun, O. K. Yusuf, P. O. Sadiku, and M. A. Shah, "Smartphone-based solution for early identification of habanero plant diseases," <em>Scientific Reports</em>, vol. 14, no. 1, p. 1423, 2024.</p>

<p>[15] D. T. N. Nhut, T. D. Tan, T. N. Quoc, and V. T. Hoang, "Medicinal plant recognition leveraging Vision Transformer and BEiT models," <em>Procedia Computer Science</em>, vol. 234, pp. 188–195, 2024.</p>

<p>[16] C. Teng, P. Yang, and M. Guo, "Report on multimodal approaches for multi-label classification," Technical Report, 2023.</p>
</div>

</body>
</html>
